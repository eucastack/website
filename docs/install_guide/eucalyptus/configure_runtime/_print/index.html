<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.99.0"><link rel=canonical type=text/html href=https://www.eucastack.io/docs/install_guide/eucalyptus/configure_runtime/><link rel=alternate type=application/rss+xml href=https://www.eucastack.io/docs/install_guide/eucalyptus/configure_runtime/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Configure the Runtime Environment | EucaStack</title><meta name=description content="Configure the Runtime Environment
After Eucalyptus is installed and registered, perform the tasks in this section to configure the runtime â€¦"><meta property="og:title" content="Configure the Runtime Environment"><meta property="og:description" content="A Docsy example site"><meta property="og:type" content="website"><meta property="og:url" content="https://www.eucastack.io/docs/install_guide/eucalyptus/configure_runtime/"><meta property="og:site_name" content="EucaStack"><meta itemprop=name content="Configure the Runtime Environment"><meta itemprop=description content="A Docsy example site"><meta name=twitter:card content="summary"><meta name=twitter:title content="Configure the Runtime Environment"><meta name=twitter:description content="A Docsy example site"><link rel=preload href=/scss/main.min.dbc832a6b41f9cb352db274e4a5e4f9087c9f9f914bcd919a25e9dad92d530f3.css as=style><link href=/scss/main.min.dbc832a6b41f9cb352db274e4a5e4f9087c9f9f914bcd919a25e9dad92d530f3.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-00000000-0","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class=navbar-logo><svg viewBox="0 0 1388.9 333.38" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g transform="translate(-136.21 41.582)"><path transform="matrix(.26458 0 0 .26458 136.21 -41.582)" d="m1616.4 487.23c-38.46.0-73.324 9.8216-104.59 29.467s-55.615 47.177-73.047 82.594c-17.155 35.14-25.732 75.123-25.732 119.95v12.451c0 66.96 19.782 120.64 59.35 161.04 39.844 40.12 91.725 60.18 155.64 60.18 37.354.0 70.971-7.3314 100.85-21.996 30.16-14.941 54.232-35.833 72.217-62.672l-54.369-51.879c-29.053 37.354-66.683 56.029-112.89 56.029-32.926.0-60.319-10.791-82.178-32.373-21.582-21.582-33.896-50.911-36.939-87.988h293.43v-40.674c0-71.387-16.741-126.59-50.221-165.6-33.203-39.014-80.378-58.52-141.53-58.52zm927.61.0c-63.086.0-112.89 20.613-149.41 61.84-36.523 40.951-54.785 95.598-54.785 163.94v10.377c0 71.663 18.399 127.83 55.199 168.51 36.8 40.674 86.605 61.01 149.41 61.01 32.373.0 62.532-7.0547 90.478-21.166 27.946-14.111 50.081-33.203 66.406-57.275 16.325-24.349 25.042-50.497 26.148-78.443h-95.045c-1.1067 21.582-9.9609 39.706-26.562 54.371-16.602 14.665-37.491 21.996-62.67 21.996-33.203.0-58.659-11.898-76.367-35.693-17.432-24.072-26.148-60.456-26.148-109.15v-16.188c.2767-48.145 9.2702-83.975 26.978-107.49 17.708-23.796 42.887-35.695 75.537-35.695 25.456.0 46.345 8.3008 62.67 24.902 16.602 16.602 25.456 37.77 26.562 63.502h95.045c-1.6602-50.635-19.368-91.448-53.125-122.44-33.48-31.266-76.921-46.898-130.32-46.898zm439.94.0c-34.587.0-65.992 6.0872-94.215 18.262-27.946 12.174-50.081 29.053-66.406 50.635-16.325 21.305-24.486 44.271-24.486 68.896h100.85c0-17.708 7.3333-32.236 21.998-43.58 14.665-11.621 33.48-17.432 56.445-17.432 26.562.0 46.345 7.0566 59.35 21.168 13.004 13.835 19.508 32.373 19.508 55.615v29.469h-61.842c-66.13.0-117.04 12.865-152.73 38.598-35.417 25.456-53.125 62.118-53.125 109.99.0 37.907 14.251 69.727 42.75 95.459s65.299 38.598 110.4 38.598c46.484.0 85.775-16.739 117.87-50.219 3.3203 19.369 7.194 33.34 11.621 41.918h102.93v-7.0547c-11.344-24.072-17.018-56.308-17.018-96.705v-202.54c-.8301-47.868-16.878-84.945-48.144-111.23-31.266-26.563-73.184-39.844-125.76-39.844zm-1099.9 8.3008v290.94c0 53.955 12.451 95.182 37.354 123.68 25.179 28.499 62.118 42.748 110.82 42.748 54.508.0 96.566-17.432 126.17-52.295l2.4903 43.994h95.045v-449.07h-100.86v322.49c-17.155 35.14-50.221 52.709-99.195 52.709-47.314.0-70.971-28.499-70.971-85.498v-289.7h-100.86zm-268.12 72.631c27.946.0 49.805 8.7168 65.576 26.148 15.772 17.155 24.765 42.611 26.978 76.367v7.4707h-192.16c4.7038-34.587 15.495-61.563 32.373-80.932 17.155-19.369 39.567-29.055 67.236-29.055zm1386.6 165.6h54.371v84.668c-8.8542 16.325-22.135 29.329-39.844 39.014-17.432 9.6842-36.109 14.527-56.031 14.527-21.582.0-38.874-5.6732-51.879-17.018-12.728-11.621-19.092-27.116-19.092-46.484.0-23.796 9.4075-42.195 28.223-55.199 18.815-13.005 46.898-19.508 84.252-19.508z" fill="#fff"/><path transform="matrix(.26458 0 0 .26458 136.21 -41.582)" d="m4889.8 307.1v637.5h100.85V798.09l44.824-46.07 133.23 192.58h116.62l-184.69-261.89 166.85-187.18h-121.19l-124.1 139.87-31.543 39.43V307.11h-100.85zm-1162.1 79.271v109.16h-73.877v74.707h73.877v254.42c0 85.498 38.737 128.25 116.21 128.25 21.305.0 43.441-3.181 66.406-9.5449v-78.027c-11.898 2.7669-23.656 4.1504-35.277 4.1504-17.432.0-29.606-3.7364-36.523-11.207-6.6406-7.7474-9.9609-20.199-9.9609-37.354v-250.68h79.272v-74.707h-79.272v-109.16h-100.86zm-296.34 100.86c-50.358.0-91.862 13.005-124.51 39.014-32.65 26.009-48.975 58.382-48.975 97.119.0 46.208 25.179 81.487 75.537 105.84 22.965 11.068 51.602 20.199 85.912 27.393 34.31 7.194 58.798 15.495 73.463 24.902 14.942 9.4076 22.412 23.103 22.412 41.088.0 16.325-6.9174 29.606-20.752 39.844-13.835 9.9609-34.033 14.941-60.596 14.941-27.392.0-49.39-6.3639-65.992-19.092-16.602-13.005-25.593-31.404-26.977-55.199h-97.949c0 26.839 8.0241 51.879 24.072 75.121 16.048 23.242 38.46 41.504 67.236 54.785s61.563 19.922 98.363 19.922c54.232.0 98.088-12.451 131.57-37.354 33.48-25.179 50.221-57.966 50.221-98.363.0-24.072-5.5338-44.548-16.602-61.426-10.791-16.878-27.532-31.266-50.221-43.164-22.412-12.174-53.402-22.551-92.969-31.129-39.567-8.8542-65.853-17.569-78.857-26.146-13.005-8.5775-19.508-20.615-19.508-36.109.0-17.155 6.9173-30.852 20.752-41.09 14.112-10.238 32.236-15.355 54.371-15.355 23.795.0 42.887 6.5013 57.275 19.506 14.388 12.728 21.582 28.776 21.582 48.145h100.86c0-42.334-16.602-76.781-49.805-103.34-32.926-26.563-76.23-39.844-129.91-39.844zm733.79.0c-34.587.0-65.99 6.0872-94.213 18.262-27.946 12.174-50.081 29.053-66.406 50.635-16.325 21.305-24.488 44.271-24.488 68.896h100.86c0-17.708 7.3313-32.236 21.996-43.58 14.665-11.621 33.48-17.432 56.445-17.432 26.562.0 46.347 7.0566 59.352 21.168 13.005 13.835 19.506 32.373 19.506 55.615v29.469h-61.84c-66.13.0-117.04 12.865-152.73 38.598-35.417 25.456-53.125 62.118-53.125 109.99.0 37.907 14.249 69.727 42.748 95.459 28.499 25.732 65.3 38.598 110.4 38.598 46.484.0 85.775-16.739 117.87-50.219 3.3202 19.369 7.194 33.34 11.621 41.918h102.93v-7.0547c-11.344-24.072-17.016-56.308-17.016-96.705v-202.54c-.83-47.868-16.878-84.945-48.144-111.23-31.266-26.563-73.186-39.844-125.76-39.844zm464.84.0c-63.086.0-112.89 20.613-149.41 61.84-36.524 40.951-54.785 95.598-54.785 163.94v10.377c0 71.663 18.401 127.83 55.201 168.51 36.8 40.674 86.605 61.01 149.41 61.01 32.373.0 62.532-7.0547 90.479-21.166 27.946-14.111 50.081-33.203 66.406-57.275 16.325-24.349 25.042-50.497 26.148-78.443h-95.045c-1.1066 21.582-9.9608 39.706-26.562 54.371-16.601 14.665-37.493 21.996-62.672 21.996-33.203.0-58.659-11.898-76.367-35.693-17.432-24.072-26.146-60.456-26.146-109.15V711.36c.2769-48.145 9.2682-83.975 26.976-107.49 17.708-23.796 42.888-35.695 75.537-35.695 25.456.0 46.347 8.3008 62.672 24.902 16.602 16.602 25.456 37.77 26.562 63.502h95.045c-1.6602-50.635-19.37-91.448-53.127-122.44-33.48-31.266-76.921-46.898-130.32-46.898zm-446.17 246.53h54.369v84.668c-8.8541 16.325-22.135 29.329-39.844 39.014-17.432 9.6842-36.107 14.527-56.029 14.527-21.582.0-38.876-5.6732-51.881-17.018-12.728-11.621-19.092-27.116-19.092-46.484.0-23.796 9.4075-42.195 28.223-55.199 18.815-13.005 46.9-19.508 84.254-19.508z" fill="#8bc13a"/></g><g transform="translate(9.5814 7.9375)"><path transform="scale(.26458)" d="m560.34-22.102-558.42 231.31c-21.06 8.7235-38.133 34.275-38.133 57.07v688.09c0 11.398 8.5364 24.173 19.067 28.535l577.49 239.2c10.53 4.3617 27.603 4.3617 38.133.0l558.42-231.31c21.06-8.7235 38.133-34.275 38.133-57.07v-688.09c0-11.398-8.5364-24.173-19.067-28.535l-577.49-239.2c-10.53-4.3617-27.603-4.3617-38.133.0z" fill="#fff"/></g><g transform="translate(-1331.7 150.96)"><g transform="translate(-.13227 -.14313)"><path transform="scale(.26458)" d="m5651.4-517.82c-4.3107-.18795-9.0505.68347-14.025 2.7441l-507.35 210.15c-19.9 8.2427-36.031 32.385-36.031 53.924v642c0 11.537 4.7163 19.84 12.072 23.934v.06641l1.0156.41992c1.147.55934 2.2859 1.1151 3.5469 1.4688l116.15 48.111v-90l410.6-170.08c19.9-8.2427 36.031-32.385 36.031-53.924v-581l108.64 45v-80l-120.26-49.812c-3.0346-1.7765-6.523-2.8394-10.385-3.0078z" fill="#8bc13a"/><path d="m1379.7 116.68V-53.18c0-5.6989 4.2682-12.087 9.5333-14.268l134.24-55.602c5.2651-2.1809 9.5333.67105 9.5333 6.3699v169.86c0 5.6989-4.2682 12.087-9.5333 14.268l-134.24 55.602c-5.2651 2.1809-9.5333-.67105-9.5333-6.3699z" fill="#003556"/><path transform="scale(.26458)" d="m5856.6-432.82c-4.3107-.18795-9.0505.68347-14.025 2.7441l-507.35 210.15c-19.9 8.2427-36.031 32.385-36.031 53.924v642c0 11.536 4.7155 19.84 12.07 23.934v.06641l1.0352.42969c1.1397.55448 2.2713 1.1073 3.5234 1.459l116.15 48.111v-90l410.6-170.08c19.9-8.2427 36.031-32.385 36.031-53.924v-561l108.64 45v-1e2l-120.26-49.812c-3.0347-1.7765-6.525-2.8394-10.387-3.0078z" fill="#8bc13a"/><path d="m1434 139.17V-30.69c0-5.6989 4.2682-12.087 9.5333-14.268l134.24-55.602c5.2651-2.1809 9.5333.67105 9.5333 6.3699v169.86c0 5.6989-4.2682 12.087-9.5333 14.268l-134.24 55.602c-5.2651 2.1809-9.5333-.67105-9.5333-6.3699z" fill="#003556"/><path transform="scale(.26458)" d="m6061.8-347.82c-4.3107-.18795-9.0505.68347-14.025 2.7441l-507.35 210.15c-19.9 8.2427-36.031 32.385-36.031 53.924v642c1e-4 11.537 4.7163 19.84 12.072 23.934v.06641l1.0156.41992c1.147.55934 2.2859 1.1151 3.5469 1.4688l116.15 48.111v-90l410.6-170.08c19.9-8.2427 36.031-32.385 36.031-53.924v-561l108.64 45v-1e2l-120.25-49.811c-3.036-1.7784-6.5287-2.8413-10.393-3.0098z" fill="#8bc13a"/><path d="m1488.3 161.66v-169.86c0-5.6989 4.2682-12.087 9.5333-14.268l134.24-55.602c5.2651-2.1809 9.5333.67105 9.5333 6.3699v169.86c0 5.6989-4.2682 12.087-9.5333 14.268l-134.24 55.602c-5.2651 2.1809-9.5333-.67105-9.5333-6.3699z" fill="#003556"/><path d="m1520.2-14.532 89.523-37.082c5.2651-2.1809 9.5333.67105 9.5333 6.3699v135.47c0 5.6989-4.2682 12.087-9.5333 14.268l-89.523 37.082c-5.2651 2.1809-9.5333-.67104-9.5333-6.3699v-135.47c0-5.6989 4.2682-12.087 9.5333-14.268z" fill="#fff"/><path d="m1596.9 85.99-63.876 26.458-1e-4-46.302 49.504-20.505V29.766l-49.504 20.505 1e-4-46.302 63.876-26.458z" fill="#8bc13a"/></g></g></svg></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=/docs/><i class="fas fa-book"></i><span class=active>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/blog/><i class="fas fa-rss"></i><span>Blog</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/community/><i class="fas fa-users"></i><span>Community</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this siteâ€¦" aria-label="Search this siteâ€¦" autocomplete=off></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/install_guide/eucalyptus/configure_runtime/>Return to the regular view of this page</a>.</p></div><h1 class=title>Configure the Runtime Environment</h1><ul><li>1: <a href=#pg-a0ef37a70072d465efc415fbe578fa67>Configure Eucalyptus DNS</a></li><ul></ul><li>2: <a href=#pg-09d0cc0a79d201e09f5c9804c4c61cd3>Create the Eucalyptus Cloud Administrator User</a></li><ul></ul><li>3: <a href=#pg-dc4447570e8060e614e4b8d90b23a14c>Upload the Network Configuration</a></li><ul></ul><li>4: <a href=#pg-fd3ae2688dc91115bc710364761b2ce5>Configure Eucalyptus Storage</a></li><ul><li>4.1: <a href=#pg-a6a6c7da8c6d5461dad3100d9ba3e31c>Configure Block Storage</a></li><ul><li>4.1.1: <a href=#pg-aa18b8d819fd6a67cb70f4fa0d18328b>Use Ceph-RBD</a></li><ul><li>4.1.1.1: <a href=#pg-30efcc5250832311a0363dcab5fce14e>Configure Hypervisor Support for Ceph-RBD</a></li><ul></ul></ul><li>4.1.2: <a href=#pg-3545c566a3fc5b961cdc7b265f1876ce>About the BROKEN State</a></li><ul></ul><li>4.1.3: <a href=#pg-07723a85c687e2a440a2f0f6cefdfba9>Use Direct Attached Storage (JBOD)</a></li><ul></ul><li>4.1.4: <a href=#pg-8aef961027c14d30b9d6272cd4fd35d4>Use the Overlay Local Filesystem</a></li><ul></ul></ul><li>4.2: <a href=#pg-25aafdcb9e624a1e783a00453ac2ce31>Configure Object Storage</a></li><ul><li>4.2.1: <a href=#pg-06a4664ccf8870894a6bb96a526ff06f>Use Ceph-RGW</a></li><ul></ul><li>4.2.2: <a href=#pg-84f727ca46a827f083dac6dafc1dba25>Use MinIO Backend</a></li><ul></ul><li>4.2.3: <a href=#pg-bfe6a73803221ee955acb39011736fbe>Use Walrus Backend</a></li><ul></ul></ul></ul><li>5: <a href=#pg-88c36c1a825c683df6b9c36b64b2ad63>Install and Configure the Imaging Service</a></li><ul></ul><li>6: <a href=#pg-ebb91e7b9d56062f2a00522894fe0965>Configure the Load Balancer</a></li><ul></ul><li>7: <a href=#pg-d2c8e2a6f2f469cba2014a65b5ab8768>Configure Node Controller</a></li><ul></ul></ul><div class=content><h1 id=configure-the-runtime-environment>Configure the Runtime Environment</h1><p>After Eucalyptus is installed and registered, perform the tasks in this section to configure the runtime environment.Now that you have installed Eucalyptus , you&rsquo;re ready to begin configuring and using it.</p></div></div><div class=td-content><h1 id=pg-a0ef37a70072d465efc415fbe578fa67>1 - Configure Eucalyptus DNS</h1><p>Eucalyptus provides a DNS service that maps service names, bucket names, and more to IP addresses. This section details how to configure the Eucalyptus DNS service.<div class="alert alert-success" role=alert><h4 class=alert-heading>Note</h4>administration tools are designed to work with DNS-enabled clouds, so configuring this service is highly recommended. The remainder of this guide is written with the assumption that your cloud is DNS-enabled.</div>The DNS service will automatically try to bind to port 53. If port 53 cannot be used, DNS will be disabled. Typically, other system services like dnsmasq are configured to run on port 53. To use the Eucalyptus DNS service, you must disable these services.</p><h2 id=configure-the-domain-and-subdomain>Configure the Domain and Subdomain</h2><p>Before using the DNS service, configure the DNS subdomain name that you want Eucalyptus to handle using the steps that follow.</p><p>Log in to the CLC and enter the following:</p><pre><code>euctl system.dns.dnsdomain=mycloud.example.com
</code></pre><p>You can configure the load balancer DNS subdomain. To do so, log in to the CLC and enter the following:</p><pre><code>euctl services.loadbalancing.dns_subdomain=lb
</code></pre><h2 id=turn-on-ip-mapping>Turn on IP Mapping</h2><p>To enable mapping of instance IPs to DNS host names:</p><p>Enter the following command on the CLC:</p><pre><code>euctl bootstrap.webservices.use_instance_dns=true
</code></pre><p>When this option is enabled, public and private DNS entries are created for each launched instance in Eucalyptus . This also enables virtual hosting for Walrus. Buckets created in Walrus can be accessed as hosts. For example, the bucket <code>mybucket</code> is accessible as <code>mybucket.objectstorage.mycloud.example.com</code> .</p><p>Instance IP addresses will be mapped as <code>euca-A-B-C-D.eucalyptus.mycloud.example.com</code> , where <code>A-B-C-D</code> is the IP address (or addresses) assigned to your instance.</p><p>If you want to modify the subdomain that is reported as part of the instance DNS name, enter the following command:</p><pre><code>euctl cloud.vmstate.instance_subdomain=.custom-dns-subdomain
</code></pre><p>When this value is modified, the public and private DNS names reported for each instance will contain the specified custom DNS subdomain name, instead of the default value, which is <code>eucalyptus</code> . For example, if this value is set to <code>foobar</code> , the instance DNS names will appear as <code>euca-A-B-C-D.foobar.mycloud.example.com</code> .</p><div class="alert alert-success" role=alert><h4 class=alert-heading>Note</h4>The code example above correctly begins with &ldquo;.&rdquo; before .</div><h2 id=enable-dns-delegation>Enable DNS Delegation</h2><p>DNS delegation allows you to forward DNS traffic for the Eucalyptus subdomain to the Eucalyptus CLC host. This host acts as a name server. This allows interruption-free access to Eucalyptus cloud services in the event of a failure. The CLC host is capable of mapping cloud host names to IP addresses of the CLC and UFS / OSG host machines.</p><p>For example, if the IP address of the CLC is <code>192.0.2.5</code> , and the IP address of Walrus is <code>192.0.2.6</code> , the host <code>compute.mycloud.example.com</code> resolves to <code>192.0.2.5</code> and <code>objectstorage.mycloud.example.com</code> resolves to <code>192.0.2.6</code> .</p><p>To enable DNS delegation:</p><p>Enter the following command on the CLC:</p><pre><code>euctl bootstrap.webservices.use_dns_delegation=true
</code></pre><h2 id=configure-the-master-dns-server>Configure the Master DNS Server</h2><p>Set up your master DNS server to delegate the Eucalyptus subdomain to the UFS host machines, which act as name servers.</p><p>The following example shows how the Linux name server <code>bind</code> is set up to delegate the Eucalyptus subdomain.</p><p>Open <em>/etc/named.conf</em> and set up the <code>example.com</code> zone. For example, your <em>/etc/named.conf</em> may look like the following:</p><pre><code>zone &quot;example.com&quot; IN {
	      type master;
	      file &quot;/etc/bind/db.example.com&quot;;
	      };
</code></pre><p>Create <em>/etc/bind/db.example.com</em> if it does not exist. If your master DNS is already set up for <code>example.com</code> , you will need to add a name server entry for UFS host machines. For example:</p><pre><code>$ORIGIN example.com.
$TTL 604800

@ IN    SOA ns1 admin.example.com 1 604800 86400 2419200 604800
        NS  ns1
ns1     A   MASTER.DNS.SERVER_IP
ufs1    A   UFS1_IP
mycloud NS  ufs1
</code></pre><p>After this, you will be able to resolve your instances&rsquo; public DNS names such as <code>euca-A-B-C-D.eucalyptus.mycloud.example.com</code> .</p><p>Restart the bind nameserver <code>service named restart</code> . Verify your setup by pointing <em>/etc/resolv.conf</em> on your client to your primary DNS server and attempt to resolve <code>compute.example.com</code> using ping or nslookup. It should return the IP address of a UFS host machine.</p><h2 id=advanced-dns-options>Advanced DNS Options</h2><p>Recursive lookups and split-horizon DNS are available in Eucalyptus .</p><p>To enable any of the DNS resolvers, set <code>dns.enabled</code> to <code>true</code> . To enable the recursive DNS resolver, set <code>dns.recursive.enabled</code> to <code>true</code> . To enable split-horizon DNS resolution for internal instance public DNS name queries, set <code>dns.split_horizon.enabled</code> to <code>true</code> .</p><h2 id=optional-configure-eucalyptus-dns-to-spoof-aws-endpoints>Optional: Configure Eucalyptus DNS to Spoof AWS Endpoints</h2><p>You can configure instances to use AWS region FQDNs for service endpoints by enabling DNS spoofing.</p><p>Set up a Eucalyptus cloud with Eucalyptus DNS and HTTPS endpoints. When creating CSR, make sure and add Subject Alternative Names for all the supported AWS services for the given region thatâ€™s being tested. For example:</p><pre><code>$ openssl req -in wildcard.c-06.autoqa.qa1.eucalyptus-systems.com.csr 
						-noout -text | less X509v3 Subject Alternative Name:
     DNS:ec2.us-east-1.amazonaws.com, DNS:autoscaling.us-east-1.amazonaws.com, 
     DNS:cloudformation.us-east-1.amazonaws.com, DNS:monitoring.us-east-1.amazonaws.com, 
     DNS:elasticloadbalancing.us-east-1.amazonaws.com, DNS:s3.amazonaws.com, 
     DNS:sts.us-east-1.amazonaws.com
</code></pre><p>Set DNS spoofing:</p><pre><code>[root@d-17 ~]#  euctl dns.spoof_regions --region euca-admin@future
dns.spoof_regions.enabled = true
dns.spoof_regions.region_name =
dns.spoof_regions.spoof_aws_default_regions = true
dns.spoof_regions.spoof_aws_regions = true
</code></pre><p>Launch an instance, and allow SSH access. SSH into the instance and install AWS CLI.</p><pre><code>ubuntu@euca-172-31-12-59:~$ sudo apt-get install -y python-pip
ubuntu@euca-172-31-12-59:~$ sudo -H pip install --upgrade pip
ubuntu@euca-172-31-12-59:~$ sudo -H pip install --upgrade awscli
</code></pre><p>Run <code>aws configure</code> and set access and secret key information if not using instance profile. Confirm AWS CLI works with HTTPS Eucalyptus service endpoint:</p><pre><code>ubuntu@euca-172-31-12-59:~$ aws --ca-bundle euca-ca-0.crt 
--endpoint-url https://ec2.c-06.autoqa.qa1.eucalyptus-systems.com/ ec2 describe-key-pairs
{
    &quot;KeyPairs&quot;: [
        {
            &quot;KeyName&quot;: &quot;devops-admin&quot;,
            &quot;KeyFingerprint&quot;: &quot;ee:4f:93:a8:87:8d:80:8d:2c:d6:d5:60:20:a3:2d:b2&quot;
        }
    ]
}
</code></pre><p>Test against AWS FQDN service endpoint that matches one of the SANs in the signed certificate:</p><pre><code>ubuntu@euca-172-31-12-59:~$ aws --ca-bundle euca-ca-0.crt 
--endpoint-url https://ec2.us-east-1.amazonaws.com ec2 describe-key-pairs{
    &quot;KeyPairs&quot;: [
        {
            &quot;KeyName&quot;: &quot;devops-admin&quot;,
            &quot;KeyFingerprint&quot;: &quot;ee:4f:93:a8:87:8d:80:8d:2c:d6:d5:60:20:a3:2d:b2&quot;
        }
    ]
}				
</code></pre></div><div class=td-content style=page-break-before:always><h1 id=pg-09d0cc0a79d201e09f5c9804c4c61cd3>2 - Create the Eucalyptus Cloud Administrator User</h1><p>After your cloud is running and DNS is functional, create a user and access key for day-to-day cloud administration.</p><h2 id=prerequisites>Prerequisites</h2><ul><li>cloud services must be installed and registered.</li><li>DNS must be configured.</li></ul><div class="alert alert-success" role=alert><h4 class=alert-heading>Note</h4>This is where you would begin using the admin role, if you want to use that feature.</div><h2 id=create-a-cloud-admin-user>Create a cloud admin user</h2><p>Eucalyptus admin tools and Euca2ools commands need configuration from <em>~/.euca</em> . If the directory does not yet exist, create it:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mkdir ~/.euca
</span></span></code></pre></div><p>Choose a name for the new user and create it along with an access key:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>euare-usercreate -wld DOMAIN USER &gt;~/.euca/FILE.ini
</span></span></code></pre></div><p>where:</p><ul><li><strong>DOMAIN</strong> must match the DNS domain for the cloud.</li><li><strong>USER</strong> is the name of the new admin user.</li><li><strong>FILE</strong> can be anything; we recommend a descriptive name that includes the user&rsquo;s name.</li></ul><p>This creates a file with a region name that matches that of your cloud&rsquo;s DNS domain; you can edit the file to change the region name if needed.</p><div class="alert alert-success" role=alert><h4 class=alert-heading>Note</h4>This creates an admin user in the built-in <strong>eucalyptus</strong> account. The admin user has full control of all aspects of the cloud. For additional security, you might instead want to create a new account and grant it access to a more limited administration role.</div><p>Switch to the new admin user:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># eval `clcadmin-release-credentials`</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># export AWS_DEFAULT_REGION=REGION</span>
</span></span></code></pre></div><p>where:</p><ul><li><strong>REGION</strong> must match the region name from the previous step. By default, this is the same as the cloud&rsquo;s DNS domain.</li></ul><p>As long as this file exists in <em>~/.euca</em> , you can use it by repeating the <code>export</code> command above. These <code>euca2ools.ini</code> configuration files are a flexible means of managing cloud regions and users.</p><p>Alternatively you can configure the default region in the <strong>global</strong> section of your Euca2ools configuration:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># cat ~/.euca/global.ini</span>
</span></span><span style=display:flex><span><span style=color:#ce5c00;font-weight:700>[</span>global<span style=color:#ce5c00;font-weight:700>]</span>
</span></span><span style=display:flex><span>default-region <span style=color:#ce5c00;font-weight:700>=</span> REGION
</span></span></code></pre></div><p>setting the <strong>REGION</strong> to the one from the earlier step means you do not have to use <em>export</em> to select the region.</p><h2 id=user-impersonation>User impersonation</h2><p>The <strong>eucalyptus</strong> account can act as other accounts for administrative purposes. To act as the <em>admin</em> user in the <em>account-1</em> account run:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># eval `clcadmin-impersonate-user -a account-1 -u admin`</span>
</span></span></code></pre></div><p>Impersonating an account allows you to view and modify resources for that account. For example, you can clean up resources in an account before deleting it.</p><p>To stop impersonating run:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>clcadmin-release-credentials
</span></span></code></pre></div><h2 id=next-steps>Next steps</h2><p>The remainder of this guide assumes you have completed the above steps.</p><p>Use these credentials after this point.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-dc4447570e8060e614e4b8d90b23a14c>3 - Upload the Network Configuration</h1><p>This topic describes how to upload the network configuration created earlier in the installation process. To upload your networking configuration:</p><div class="alert alert-success" role=alert><h4 class=alert-heading>Note</h4>This step can only be run after getting your credentials in <a href=https://www.eucastack.io/docs/install_guide/eucalyptus/configure_runtime/credentials_admin_create/>Create the Eucalyptus Cloud Administrator User</a>.</div><p>Run the following command to upload the configuration file to the CLC (with valid Eucalyptus admin credentials):</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>euctl cloud.network.network_configuration<span style=color:#ce5c00;font-weight:700>=</span>@/path/to/your/network_config_file
</span></span></code></pre></div><p>To review the existing network configuration run:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>euctl  --dump --format<span style=color:#ce5c00;font-weight:700>=</span>raw cloud.network.network_configuration
</span></span></code></pre></div><p>When you use the Ansible playbook for deployment a network configuration file is available at <em>/etc/eucalyptus/network.yaml</em> on the CLC.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-fd3ae2688dc91115bc710364761b2ce5>4 - Configure Eucalyptus Storage</h1><h1 id=configure-storage>Configure Storage</h1><p>These are the types of storage available for your Eucalyptus cloud. Object storage Eucalyptus provides an AWS S3 compatible object storage service that provides users with web-based general purpose storage, designed to be scalable, reliable and inexpensive. You choose the object storage backend provider: Walrus or Ceph RGW. The Object Storage Gateway (OSG) provides access to objects via the backend provider you choose.</p><p>Block storage Eucalyptus provides an AWS EBS compatible block storage service that provides block storage for EC2 instances. Volumes can be created as needed and dynamically attached and detached to instances as required. EBS provides persistent data storage for instances: the volume, and the data on it, can exist beyond the lifetime of an instance. You choose the block storage backend provider for a deployment.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-a6a6c7da8c6d5461dad3100d9ba3e31c>4.1 - Configure Block Storage</h1><h1 id=configure-block-storage>Configure Block Storage</h1><p>This topic describes how to configure block storage on the Storage Controller (SC) for the backend of your choice.</p><p>The Storage Controller (SC) provides functionality similar to the Amazon Elastic Block Store (Amazon EBS). The SC can interface with various storage systems. Eucalyptus block storage (EBS) exports storage volumes that can be attached to a VM and mounted or accessed as a raw block device. EBS volumes can persist past VM termination and are commonly used to store persistent data.</p><p>Eucalyptus provides the following open source (free) backend providers for the SC:</p><ul><li>Overlay, using the local file system</li><li>DAS-JBOD (just a bunch of disks)</li><li>Ceph</li></ul><p>You must configure the SC to use one of the backend provider options.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-aa18b8d819fd6a67cb70f4fa0d18328b>4.1.1 - Use Ceph-RBD</h1><h1 id=use-ceph-rbd>Use Ceph-RBD</h1><p>This topic describes how to configure Ceph-RBD as the block storage backend provider for the Storage Controller (SC).<strong>Prerequisites</strong></p><ul><li><p>Successful completion of all the install sections prior to this section.</p></li><li><p>The SC must be installed, registered, and running.</p></li><li><p>You must execute the steps below as a administrator.</p></li><li><p>You must have a functioning Ceph cluster.</p></li><li><p>Ceph user credentials with the following privileges are available to SCs and NCs (different user credentials can be used for the SCs and NCs).</p></li><li><p>Hypervisor support for Ceph-RBD on NCs. Node Controllers (NCs) are designed to communicate with the Ceph cluster via libvirt. This interaction requires a hypervisor that supports Ceph-RBD. See to satisfy this prerequisite.
<strong>To configure Ceph-RBD block storage for the zone, run the following commands on the CLC</strong> Configure the SC to use Ceph-RBD for EBS.</p><p>euctl ZONE.storage.blockstoragemanager=ceph-rbd</p></li></ul><p>The output of the command should be similar to:</p><pre><code>one.storage.blockstoragemanager=ceph-rbd
</code></pre><p>Verify that the property value is now <code>ceph-rbd</code> :</p><pre><code>euctl ZONE.storage.blockstoragemanager
</code></pre><p>Check the SC to be sure that it has transitioned out of the <code>BROKEN</code> state and is in the <code>NOTREADY</code> , <code>DISABLED</code> or <code>ENABLED</code> state before configuring the rest of the properties for the SC. The ceph-rbd provider will assume defaults for the following properties for the SC:</p><pre><code>euctl ZONE.storage.ceph
 
PROPERTY        one.storage.cephconfigfile  /etc/ceph/ceph.conf
DESCRIPTION     one.storage.cephconfigfile  Absolute path to Ceph configuration (ceph.conf) file. Default value is '/etc/ceph/ceph.conf'
 
PROPERTY        one.storage.cephkeyringfile /etc/ceph/ceph.client.eucalyptus.keyring
DESCRIPTION     one.storage.cephkeyringfile Absolute path to Ceph keyring (ceph.client.eucalyptus.keyring) file. Default value is '/etc/ceph/ceph.client.eucalyptus.keyring'
 
PROPERTY        one.storage.cephsnapshotpools       rbd
DESCRIPTION     one.storage.cephsnapshotpools       Ceph storage pool(s) made available to  for EBS snapshots. Use a comma separated list for configuring multiple pools. Default value is 'rbd'
 
PROPERTY        one.storage.cephuser        eucalyptus
DESCRIPTION     one.storage.cephuser        Ceph username employed by  operations. Default value is 'eucalyptus'
 
PROPERTY        one.storage.cephvolumepools rbd
DESCRIPTION     one.storage.cephvolumepools Ceph storage pool(s) made available to  for EBS volumes. Use a comma separated list for configuring multiple pools. Default value is 'rbd'
</code></pre><p>The following steps are optional if the default values do not work for your cloud: To set the Ceph username (the default value for Eucalyptus is &rsquo;eucalyptus&rsquo;):</p><pre><code>euctl ZONE.storage.cephuser=myuser
</code></pre><p>To set the absolute path to keyring file containing the key for the &rsquo;eucalyptus&rsquo; user (the default value is &lsquo;/etc/ceph/ceph.client.eucalyptus.keyring&rsquo;):</p><pre><code>euctl ZONE.storage.cephkeyringfile='/etc/ceph/ceph.client.myuser.keyring'
</code></pre><p><div class="alert alert-success" role=alert><h4 class=alert-heading>Note</h4>If cephuser was modified, ensure that cephkeyringfile is also updated with the location to the keyring for the specific cephuser:</div>To set the absolute path to ceph.conf file (default value is &lsquo;/etc/ceph/ceph.conf&rsquo;):</p><pre><code>euctl ZONE.storage.cephconfigfile=/path/to/ceph.conf
</code></pre><p>To change the comma-delimited list of Ceph pools assigned to Eucalyptus for managing EBS volumes (default value is &lsquo;rbd&rsquo;) :</p><pre><code>euctl ZONE.storage.cephvolumepools=rbd,myvolumes
</code></pre><p>To change the comma-delimited list of Ceph pools assigned to Eucalyptus for managing EBS snapshots (default value is &lsquo;rbd&rsquo;) :</p><pre><code>euctl ZONE.storage.cephsnapshotpools=mysnapshots
</code></pre><p>If you want to enable snapshot deltas for your Ceph backend:<div class="alert alert-success" role=alert><h4 class=alert-heading>Note</h4>Snapshot deltas are supported only on Ceph-RBD.</div></p><p><div class="alert alert-success" role=alert><h4 class=alert-heading>Note</h4></div>Verify that snapshots are enabled:</p><pre><code>euctl ZONE.storage.shouldtransfersnapshots=true
</code></pre><p>Set the maximum number of deltas to be created before creating a new full snapshot:</p><pre><code>euctl ZONE.storage.maxsnapshotdeltas=NON_ZERO_INTEGER
</code></pre><div class="alert alert-success" role=alert><h4 class=alert-heading>Note</h4>This variable applies to all Ceph volumes.</div><p><div class="alert alert-success" role=alert><h4 class=alert-heading>Note</h4>If you need to create a non-Ceph volume from a Ceph snapshot, this value would need to be set to zero (at least temporarily).</div>Every NC will assume the following defaults:</p><pre><code>CEPH_USER_NAME=&quot;eucalyptus&quot;
CEPH_KEYRING_PATH=&quot;/etc/ceph/ceph.client.eucalyptus.keyring&quot;
CEPH_CONFIG_PATH=&quot;/etc/ceph/ceph.conf&quot;
</code></pre><p>To override the above defaults, add/edit the following properties in the <code>/etc/eucalyptus/eucalyptus.conf</code> on the specific NC file:</p><pre><code>CEPH_USER_NAME=&quot;ceph-username-for-use-by-this-NC&quot;
CEPH_KEYRING_PATH=&quot;path-to-keyring-file-for-ceph-username&quot;
CEPH_CONFIG_PATH=&quot;path-to-ceph.conf-file&quot;
</code></pre><p>Repeat this step for every NC in the specific Eucalyptus zone. Your Ceph backend is now ready to use with Eucalyptus .</p></div><div class=td-content style=page-break-before:always><h1 id=pg-30efcc5250832311a0363dcab5fce14e>4.1.1.1 - Configure Hypervisor Support for Ceph-RBD</h1><p>This topic describes how to configure the hypervisor for Ceph-RBD support.The following instructions will walk you through steps for verifying and or installing the required hypervisor for Ceph-RBD support. <strong>Repeat this process for every NC in the Eucalyptus zone</strong></p><p>Verify if <code>qemu-kvm</code> and <code>qemu-img</code> are already installed.</p><pre><code>rpm -q qemu-kvm qemu-img
</code></pre><p>Proceed to the preparing the RHEV qemu packages step if they are not installed.</p><p>Verify qemu support for the <code>ceph-rbd</code> driver.</p><pre><code>qemu-img --help
qemu-img version 0.12.1, Copyright (c) 2004-2008 Fabrice Bellard
...
Supported formats: raw cow qcow vdi vmdk cloop dmg bochs vpc vvfat qcow2 qed vhdx parallels nbd blkdebug host_cdrom 
host_floppy host_device file gluster gluster gluster gluster rbd
</code></pre><p><div class="alert alert-success" role=alert><h4 class=alert-heading>Note</h4>If &lsquo;rbd&rsquo; is listed as one of the supported formats, no further action is required; otherwise proceed to the next step.</div>If the <code>eucalyptus-node</code> service is running, terminate/stop all instances. After all instances are terminated, stop the eucalyptus-node service.</p><pre><code>systemctl stop eucalyptus-node.service
</code></pre><p>Prepare the RHEV qemu packages:</p><ul><li><p>If this NC is a RHEL system and the RHEV subscription to qemu packages is available, consult the RHEV package procedure to install the qemu-kvm-ev and qemu-img-ev packages. Blacklist the RHEV packages in the repository to ensure that packages from the RHEV repository are installed.</p></li><li><p>If this NC is a RHEL system and RHEV subscription to qemu packages is unavailable, built and maintained qemu-rhev packages may be used. These packages are available in the same yum repository as other packages. Note that using built RHEV packages voids the original RHEL support for the qemu packages.</p></li><li><p>If this NC is a non-RHEL (CentOS) system, -built and maintained qemu-rhev packages may be used. These packages are available in the same yum repository as other packages.
If you are <em>not</em> using the RHEV package procedure to install the <code>qemu-kvm-ev</code> and <code>qemu-img-ev</code> packages, install Eucalyptus -built RHEV packages: <code>qemu-kvm-ev</code> and <code>qemu-img-ev</code> , which can be found in the same yum repository as other Eucalyptus packages.</p><p>yum install qemu-kvm-ev qemu-img-ev</p></li></ul><p>Start the <code>libvirtd</code> service.</p><pre><code>systemctl start libvirtd.service
</code></pre><p>Verify <code>qemu</code> support for the <code>ceph-rbd</code> driver.</p><pre><code>qemu-img --help
qemu-img version 0.12.1, Copyright (c) 2004-2008 Fabrice Bellard
...
Supported formats: raw cow qcow vdi vmdk cloop dmg bochs vpc vvfat qcow2 qed vhdx parallels nbd blkdebug host_cdrom 
host_floppy host_device file gluster gluster gluster gluster rbd
</code></pre><p>Make sure the eucalyptus-node service is started.</p><pre><code>systemctl start eucalyptus-node.service
</code></pre><p>Your hypervisor is ready for Eucalyptus Ceph-RBD support. You are now ready to <a href=https://www.eucastack.io/docs/install_guide/eucalyptus/configure_runtime/config_storage/config_block_storage/config_storage_ceph_rbd/>configure Ceph-RBD</a> for Eucalyptus .</p></div><div class=td-content style=page-break-before:always><h1 id=pg-3545c566a3fc5b961cdc7b265f1876ce>4.1.2 - About the BROKEN State</h1><p>This topic describes the initial state of the Storage Controller (SC) after you have registered it with the Cloud Controller (CLC).The SC automatically goes to the <code>broken</code> state after being registered with the CLC; it will remain in that state until you explicitly configure the SC by telling it which backend storage provider to use.</p><p>You can check the state of a storage controller by running <code>euserv-describe-services --expert</code> and note the state and status message of the SC(s). The output for an unconfigured SC looks something like this:</p><pre><code>SERVICE	storage        	ZONE1        	SC71           	BROKEN    	37  	http://192.168.51.71:8773/services/Storage	arn:euca:eucalyptus:ZONE1:storage:SC71/
SERVICEEVENT	6c1f7a0a-21c9-496c-bb79-23ddd5749222	arn:euca:eucalyptus:ZONE1:storage:SC71/
SERVICEEVENT	6c1f7a0a-21c9-496c-bb79-23ddd5749222	ERROR
SERVICEEVENT	6c1f7a0a-21c9-496c-bb79-23ddd5749222	Sun Nov 18 22:11:13 PST 2012
SERVICEEVENT	6c1f7a0a-21c9-496c-bb79-23ddd5749222	SC blockstorageamanger not configured. Found empty or unset manager(unset). Legal values are: das,overlay,ceph
</code></pre><p>Note the error above: <code>SC blockstoragemanager not configured. Found empty or unset manager(unset). Legal values are: das,overlay,ceph</code> .</p><p>This indicates that the SC is not yet configured. It can be configured by setting the <code>ZONE.storage.blockstoragemanager</code> property to &lsquo;das&rsquo;, &lsquo;overlay&rsquo;, or &lsquo;ceph&rsquo;.</p><p>You can verify that the configured SC block storage manager using:</p><pre><code>euctl ZONE.storage.blockstoragemanager
</code></pre><p>to show the current value.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-07723a85c687e2a440a2f0f6cefdfba9>4.1.3 - Use Direct Attached Storage (JBOD)</h1><p>This topic describes how to configure the DAS-JBOD as the block storage backend provider for the Storage Controller (SC).<strong>Prerequisites</strong></p><ul><li><p>Successful completion of all the install sections prior to this section.</p></li><li><p>The SC must be installed, registered, and running.</p></li><li><p>Direct Attached Storage requires that have enough space for locally cached snapshots.</p></li><li><p>You must execute the steps below as a administrator.
<strong>To configure DAS-JBOD block storage for the zone, run the following commands on the CLC</strong> Configure the SC to use the Direct Attached Storage for EBS.</p><p>euctl ZONE.storage.blockstoragemanager=das</p></li></ul><p>The output of the command should be similar to:</p><pre><code>one.storage.blockstoragemanager=das
</code></pre><p>Verify that the property value is now: &lsquo;das&rsquo;</p><pre><code>euctl ZONE.storage.blockstoragemanager
</code></pre><p>Set the DAS device name property. The device name can be either a raw device (/dev/sdX, for example), or the name of an existing Linux LVM volume group.</p><pre><code>euctl ZONE.storage.dasdevice=DEVICE_NAME
</code></pre><p>For example:</p><pre><code>euctl one.storage.dasdevice=/dev/sdb
</code></pre><p>Your DAS-JBOD backend is now ready to use with Eucalyptus .</p></div><div class=td-content style=page-break-before:always><h1 id=pg-8aef961027c14d30b9d6272cd4fd35d4>4.1.4 - Use the Overlay Local Filesystem</h1><p>This topic describes how to configure the local filesystem as the block storage backend provider for the Storage Controller (SC).<strong>Prerequisites</strong></p><ul><li>Successful completion of all the install sections prior to this section.</li><li>The SC must be installed, registered, and running.</li><li>The local filesystem must have enough space to hold volumes and snapshots created in the cloud.</li><li>You must execute the steps below as a administrator.
In this configuration the SC itself hosts the volume and snapshots for EBS and stores them as files on the local filesystem. It uses standard Linux iSCSI tools to serve the volumes to instances running on NCs.</li></ul><p><strong>To configure overlay block storage for the zone, run the following commands on the CLC</strong> Configure the SC to use the local filesystem for EBS.</p><pre><code>euctl ZONE.storage.blockstoragemanager=overlay 
</code></pre><p>The output of the command should be similar to:</p><pre><code>one.storage.blockstoragemanager=overlay
</code></pre><p>Verify that the property value is now: &lsquo;overlay&rsquo;</p><pre><code>euctl ZONE.storage.blockstoragemanager
</code></pre><p>Your local filesystem (overlay) backend is now ready to use with Eucalyptus .</p></div><div class=td-content style=page-break-before:always><h1 id=pg-25aafdcb9e624a1e783a00453ac2ce31>4.2 - Configure Object Storage</h1><p>This topic describes how to configure object storage on the Object Storage Gateway (OSG) for the backend of your choice. The OSG passes requests to object storage providers and talks to the persistence layer (DB) to authenticate requests. You can use Walrus, MinIO, or Ceph-RGW as the object storage provider.</p><ul><li><p><strong>Walrus</strong> - the default backend provider. It is a single-host Eucalyptus -integrated provider which provides basic object storage functionality for the small scale. Walrus is intended for light S3 usage.</p></li><li><p><strong>MinIO</strong> - a high performing scalable object storage provider. MinIO implements the S3 API which is used by the OSG, not directly by end users. Distributed MinIO provides protection against multiple node/drive failures and bit rot using erasure code.</p></li><li><p><strong>Ceph-RGW</strong> - an object storage interface built on top of Librados to provide applications with a RESTful gateway to Ceph Storage Clusters. Ceph-RGW uses the Ceph Object Gateway daemon (radosgw), which is a FastCGI module for interacting with a Ceph Storage Cluster. Since it provides interfaces compatible with OpenStack Swift and Amazon S3, the Ceph Object Gateway has its own user management. Ceph Object Gateway can store data in the same Ceph Storage Cluster used to store data from Ceph Filesystem clients or Ceph Block Device clients. The S3 and Swift APIs share a common namespace, so you may write data with one API and retrieve it with the other.</p></li></ul><p>You must configure the OSG to use one of the backend provider options.</p><div class="alert alert-success" role=alert><h4 class=alert-heading>Note</h4>If OSG has been registered but not yet properly configured, it will be listed in the state when listed with the euserv-describe-services command.</div><p>Example showing unconfigured <em>objectstorage</em>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># euserv-describe-services --show-headers --filter service-type=objectstorage</span>
</span></span><span style=display:flex><span>SERVICE  TYPE              	ZONE    	NAME                   	  STATE	
</span></span><span style=display:flex><span>SERVICE  objectstorage      user-api-1  user-api-1.objectstorage  broken
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-06a4664ccf8870894a6bb96a526ff06f>4.2.1 - Use Ceph-RGW</h1><p>This topic describes how to configure Ceph Rados Gateway (RGW) as the backend for the Object Storage Gateway (OSG).</p><h2 id=prerequisites>Prerequisites</h2><ul><li>Successful completion of all the install sections prior to this section.</li><li>The UFS must be registered and enabled.</li><li>A Ceph storage cluster is available.</li><li>The ceph-radosgw service has been installed (on the UFS or any other host) and configured to use the Ceph storage cluster. recommends using civetweb with ceph-radosgw service. is a lightweight web server and is included in the ceph-radosgw installation. It is relatively easier to install and configure than the alternative option â€“ a combination of Apache and Fastcgi modules.</li></ul><p>For more information on Ceph-RGW, see the <a href=http://docs.ceph.com/docs/master/radosgw/>Ceph-RGW documentation</a>.</p><h2 id=configure-ceph-rgw-object-storage>Configure Ceph-RGW object storage</h2><p>You must execute the steps below as a administrator.</p><p>Configure <strong>ceph-rgw</strong> as the storage provider using the <em>euctl</em> command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>euctl objectstorage.providerclient<span style=color:#ce5c00;font-weight:700>=</span>ceph-rgw
</span></span></code></pre></div><p>Configure <em>objectstorage.s3provider.s3endpoint</em> to the <strong>ip:port</strong> of the host running the ceph-radosgw service:</p><div class="alert alert-success" role=alert><h4 class=alert-heading>Note</h4>Depending on the front end web server used by ceph-radosgw service, the default port is 80 for apache and 7480 for civetweb.</div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>euctl objectstorage.s3provider.s3endpoint<span style=color:#ce5c00;font-weight:700>=</span>&lt;radosgw-host-ip&gt;:&lt;radosgw-webserver-port&gt;
</span></span></code></pre></div><p>Configure <em>objectstorage.s3provider.s3accesskey</em> and <em>objectstorage.s3provider.s3secretkey</em> with the radosgw user credentials:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>euctl objectstorage.s3provider.s3accesskey<span style=color:#ce5c00;font-weight:700>=</span>&lt;radosgw-user-accesskey&gt;
</span></span><span style=display:flex><span>euctl objectstorage.s3provider.s3secretkey<span style=color:#ce5c00;font-weight:700>=</span>&lt;radosgw-user-secretkey&gt;
</span></span></code></pre></div><p>The Ceph-RGW backend and OSG are now ready for production.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-84f727ca46a827f083dac6dafc1dba25>4.2.2 - Use MinIO Backend</h1><p>This topic describes how to configure MinIO as the object storage backend provider for the Object Storage Gateway (OSG).</p><h2 id=prerequisites>Prerequisites</h2><ul><li>The UFS must be registered and enabled.</li><li>Install and start MinIO</li></ul><p>For more information on MinIO installation and configuration see the <a href=https://docs.min.io/>MinIO Server Documentation</a></p><h2 id=to-configure-minio-object-storage>To configure MinIO object storage</h2><p>You must execute the steps below as a administrator.</p><p>Configure <strong>minio</strong> as the storage provider using the <em>euctl</em> command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>euctl objectstorage.providerclient<span style=color:#ce5c00;font-weight:700>=</span>minio
</span></span></code></pre></div><p>Configure <em>objectstorage.s3provider.s3endpoint</em> to the <strong>ip:port</strong> of a host running the minio server:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>euctl objectstorage.s3provider.s3endpoint<span style=color:#ce5c00;font-weight:700>=</span>&lt;minio-host-ip&gt;:&lt;minio-port&gt;
</span></span></code></pre></div><div class="alert alert-success" role=alert><h4 class=alert-heading>Note</h4>The default port for MinIO is 9000</div><p>Configure <em>objectstorage.s3provider.s3accesskey</em> and <em>objectstorage.s3provider.s3secretkey</em> with credentials for minio:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>euctl objectstorage.s3provider.s3accesskey<span style=color:#ce5c00;font-weight:700>=</span>&lt;minio-accesskey&gt;
</span></span><span style=display:flex><span>euctl objectstorage.s3provider.s3secretkey<span style=color:#ce5c00;font-weight:700>=</span>&lt;minio-secretkey&gt;
</span></span></code></pre></div><p>Configure the expected response code for minio:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>euctl objectstorage.s3provider.s3endpointheadresponse<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>400</span>
</span></span></code></pre></div><p>The MinIO backend and OSG are now ready for production.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-bfe6a73803221ee955acb39011736fbe>4.2.3 - Use Walrus Backend</h1><p>This topic describes how to configure Walrus as the object storage backend provider for the Object Storage Gateway (OSG).</p><h2 id=prerequisites>Prerequisites</h2><ul><li>Successful completion of all the install sections prior to this section.</li><li>The UFS must be registered and enabled.</li></ul><h2 id=to-configure-walrus-object-storage>To configure Walrus object storage</h2><p>You must execute the steps below as a administrator.</p><p>Configure <strong>walrus</strong> as the storage provider using the <em>euctl</em> command.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>euctl objectstorage.providerclient<span style=color:#ce5c00;font-weight:700>=</span>walrus
</span></span></code></pre></div><p>Check that the OSG is enabled.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>euserv-describe-services
</span></span></code></pre></div><p>If the state appears as <strong>disabled</strong> or <strong>broken</strong> , check the cloud-*.log files in the <em>/var/log/eucalyptus</em> directory. A <strong>disabled</strong> state generally indicates that there is a problem with your network or credentials. See <a href=https://www.eucastack.io/docs/admin_guide/ops_oview/ops_ts/ts_logs/>Log File Location and Content</a> for more information.</p><p>The Walrus backend and OSG are now ready for production.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-88c36c1a825c683df6b9c36b64b2ad63>5 - Install and Configure the Imaging Service</h1><p>The Eucalyptus Imaging Service, introduced in Eucalyptus 4.0, makes it easier to deploy EBS images in your Eucalyptus cloud and automates many of the labor-intensive processes required for uploading data into EBS images.</p><p>The Eucalyptus Imaging Service is implemented as a system-controlled &ldquo;worker&rdquo; virtual machine that is monitored and controlled via Auto Scaling. Once the Imaging Service is configured, the Imaging Service VM will be started automatically upon the first request that requires it: such as an EBS volume ingress. Specifically, in this release of Eucalyptus , these are the usage scenarios for the Eucalyptus Imaging Service:</p><ul><li><strong>Importing a raw disk image as a volume:</strong> If you have a raw disk image (containing either a data partition or a full operating system with a boot record, e.g., an HVM image), you can use the Imaging Service to import this into your cloud as a volume. This is accomplished with the <code>euca-import-volume</code> command. If the volume was populated with a bootable disk, that volume can be snapshotted and registered as an image.</li><li><strong>Importing a raw disk image as an instance:</strong> If you have a raw disk image containing a bootable operating system, you can import this disk image into as an instance: the Imaging Service automatically creates a volume, registers the image, and launches an instance from the image. This is accomplished with the <code>euca-import-instance</code> command, which has options for specifying the instance type and the SSH key for the instance to use.</li></ul><h2 id=install-and-register-the-imaging-worker-image>Install and Register the Imaging Worker Image</h2><p>Eucalyptus provides a command-line tool for installing and registering the Imaging Worker image. Once you have run the tool, the Imaging Worker will be ready to use.Run the following commands on the machine where you installed the <code>eucalyptus-service-image</code> RPM package (it will set the <code>imaging.imaging_worker_emi</code> property to the newly created EMI of the imaging worker):</p><pre><code>esi-install-image --region localhost --install-default
</code></pre><p>Consider setting the <code>imaging.imaging_worker_keyname</code> property to an SSH keyname (previously created with the <code>euca-create-keypair</code> command), so that you can perform troubleshooting inside the Imaging Worker instance, if necessary:</p><pre><code>euctl services.imaging.worker.keyname=mykey
</code></pre><h2 id=managing-the-imaging-worker-instance>Managing the Imaging Worker Instance</h2><p>Eucalyptus automatically starts Imaging Worker instances when there are tasks for workers to perform.The cloud administrator can list the running Imaging Worker instances, if any, by running the command:</p><pre><code>euca-describe-instances --filter tag-value=euca-internal-imaging-workers
</code></pre><p>To delete / stop the imaging worker:</p><pre><code>esi-manage-stack -a delete imaging
</code></pre><p>To create / start the imaging worker:</p><pre><code>esi-manage-stack -a create imaging
</code></pre><p>Consider setting the <code>imaging.imaging_worker_instance_type</code> property to an Instance Type with enough ephemeral disk to convert any of your paravirtual images. The Imaging Worker root filesystem takes up about 2GB, so the maximum paravirtual image that the Imaging Worker will be able to convert is the disk allocation of the Instance Type minus 2GBs.</p><pre><code>euctl services.imaging.worker.instance_type=m3.xlarge
</code></pre><h2 id=troubleshooting-imaging-worker>Troubleshooting Imaging Worker</h2><p>If the Imaging Worker is configured correctly, users will be able to import data into EBS volumes with <code>euca-import-*</code> commands, and paravirtual EMIs will run as instances. In some cases, though, paravirtual images may fail to convert (e.g., due to intermittent network failures or a network setup that doesn&rsquo;t allow the Imaging Worker to communicate with the CLC), leaving the images in a special state. To troubleshoot:If the Imaging Worker Instance Type does not provide sufficient disk space for converting all paravirtual images, the administrator may have to change the Instance Type used by the Imaging Worker. After changing the instance type, the Imaging Worker instance should be restarted by terminating the old Imaging Worker instance:</p><pre><code>euctl services.imaging.worker.instance_type=m2.2xlarge
euca-terminate-instances $(euca-describe-instances --filter tag-value=euca-internal-imaging-workers | grep INSTANCE | cut -f 2)
</code></pre><p>If the status of the conversion operation is &lsquo;Image conversion failed&rsquo;, but the image is marked as &lsquo;available&rsquo; (in the output of euca-describe-images), the conversion can be retried by running the EMI again:</p><pre><code>euca-run-instances ...
</code></pre></div><div class=td-content style=page-break-before:always><h1 id=pg-ebb91e7b9d56062f2a00522894fe0965>6 - Configure the Load Balancer</h1><h2 id=install-and-register-the-load-balancer-image>Install and Register the Load Balancer Image</h2><p>Eucalyptus provides a tool for installing and registering the Load Balancer image. Once you have run the tool, your Load Balancer will be ready to use.</p><p>Run the following commands on the machine where you installed the <code>eucalyptus-service-image</code> RPM package (it will set the <code>imaging.imaging_worker_emi</code> property to the newly created EMI of the imaging worker):</p><pre><code>esi-install-image --install-default
</code></pre><h2 id=verify-load-balancer-configuration>Verify Load Balancer Configuration</h2><p>If you would like to verify that Load Balancer support is enabled you can list installed Load Balancers. The currently active Load Balancer will be listed as enabled. If no Load Balancers are listed, or none are marked as enabled, then your Load Balancer support has not been configured properly.Run the following command to list installed Load Balancer images:</p><pre><code>esi-describe-images
</code></pre><p>This will produce output similar to the followin:</p><pre><code>SERVICE     VERSION  ACTIVE     IMAGE      INSTANCES
    imaging       2.2      *     emi-573925e5      0
 loadbalancing    2.2      *     emi-573925e5      0
    database      2.2      *     emi-573925e5      0
</code></pre><p>You can also check the enabled Load Balancer EMI with:</p><pre><code>euctl services.loadbalancing.worker.image
</code></pre><p>If you need to manually set the enabled Load Balancer EMI use:</p><pre><code>euctl services.loadbalancing.worker.image=emi-12345678
</code></pre></div><div class=td-content style=page-break-before:always><h1 id=pg-d2c8e2a6f2f469cba2014a65b5ab8768>7 - Configure Node Controller</h1><p>On some Linux installations, a sufficiently large amount of local disk activity can slow down process scheduling. This can cause other operations (e.g., network communication and instance provisioning) appear to stall. Examples of disk-intensive operations include preparing disk images for launch and creating ephemeral storage.</p><ol><li>Log in to an NC server and open the <em>/etc/eucalyptus/eucalyptus.conf</em> file.</li><li>Change the <code>CONCURRENT_DISK_OPS</code> parameter to the number of disk-intensive operations you want the NC to perform at once.<ol><li>Set <code>CONCURRENT_DISK_OPS</code> to 1 to serialize all disk-intensive operations. Or &mldr;</li><li>Set it to a higher number to increase the amount of disk-intensive operations the NC will perform in parallel.</li></ol></li></ol></div></main></div></div><footer class="bg-dark py-5 row d-print-none"><div class="container-fluid mx-sm-5"><div class=row><div class="col-6 col-sm-4 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Community forums" aria-label="Community forums"><a class=text-white target=_blank rel=noopener href=https://github.com/orgs/eucastack/discussions aria-label="Community forums"><i class="fa fa-comments"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank rel=noopener href=https://twitter.com/eucastack aria-label=Twitter><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank rel=noopener href=https://stackoverflow.com/questions/tagged/eucastack aria-label="Stack Overflow"><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-6 col-sm-4 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank rel=noopener href=https://github.com/eucastack aria-label=GitHub><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Discord aria-label=Discord><a class=text-white target=_blank rel=noopener href=https://discord.gg/A9UPtck4 aria-label=Discord><i class="fab fa-discord"></i></a></li></ul></div><div class="col-12 col-sm-4 text-center py-2 order-sm-2"><small class=text-white>&copy; 2022 EucaStack All Rights Reserved</small>
<small class=ml-1><a href=https://policies.google.com/privacy target=_blank rel=noopener>Privacy Policy</a></small></div></div></div></footer></div><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA==" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script>
<script src=/js/main.min.6f618f935d0e212fc39316b4023fa69460fd4a5779211bdc3535ec542578f225.js integrity="sha256-b2GPk10OIS/Dkxa0Aj+mlGD9Sld5IRvcNTXsVCV48iU=" crossorigin=anonymous></script></body></html>